{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train (length=3, w_0=0.5)",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yje9hqtcUQ_f",
        "colab_type": "text"
      },
      "source": [
        "### Initialization\n",
        "* Check whether the runtime is host or local.\n",
        "* Mount Google Drive when using the host runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwqGy_GyUQnw",
        "colab_type": "code",
        "outputId": "103f684d-18df-463d-9d36-5c3a7265050e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/gdrive')\n",
        "  runtime = \"host\"\n",
        "except:\n",
        "  runtime = \"local\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S457sT6QMUr",
        "colab_type": "text"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QN-4eF51DNqt",
        "colab": {}
      },
      "source": [
        "#@title Parameters\n",
        "#@markdown |Name            |Description|\n",
        "#@markdown |:---            |:---|\n",
        "#@markdown |`seed`|The random seed|\n",
        "seed = 3984 #@param {type: \"number\"}\n",
        "\n",
        "#@markdown ### `deep-coder` Repositories\n",
        "#@markdown |Name            |Description|\n",
        "#@markdown |:---            |:---|\n",
        "#@markdown |`repository_url`|The URL of `deep-coder` git repository (enabled only in the host runtime)|\n",
        "#@markdown |`branch_name`   |The branch name (enabled only in the host runtime)|\n",
        "repository_url = \"https://github.com/HiroakiMikami/deep-coder\" #@param {type: \"string\"}\n",
        "branch_name = \"master\" #@param {type: \"string\"}\n",
        "\n",
        "#@markdown ### Model Parameters\n",
        "#@markdown |Name               |Description|\n",
        "#@markdown |:---               |:---|\n",
        "#@markdown |`n_embed`          |The dimension of integer embeddings|\n",
        "#@markdown |`n_units`          |The number of units in the hidden layers|\n",
        "#@markdown |`num_hidden_layers`|The number of the hidden layers|\n",
        "n_embed = 20 #@param {type: \"number\"}\n",
        "n_units = 256 #@param {type: \"number\"}\n",
        "num_hidden_layers = 3 #@param {type: \"number\"}\n",
        "\n",
        "#@markdown ### Training Settings\n",
        "#@markdown |Name                |Description|\n",
        "#@markdown |:---                |:---|\n",
        "#@markdown |`batch_size`        |The minibatch size|\n",
        "#@markdown |`weight_label_false`|The weight for the loss value in the case of attribute=False. `-1` means that using the original loss function|\n",
        "#@markdown |`num_epochs`        |The numer of epoch|\n",
        "#@markdown |`ratio_test`        |The ratio of entries for testing|\n",
        "#@markdown |`num_train`         |The number of entries used for training|\n",
        "batch_size = 32 #@param {type: \"number\"}\n",
        "weight_label_false = 0.5 #@param {type: \"number\"}\n",
        "num_epochs = 10 #@param {type: \"number\"}\n",
        "ratio_test = 0 #@param {type: \"number\"}\n",
        "num_train = 0 #@param {type: \"number\"}\n",
        "\n",
        "#@markdown ### Validation Settings\n",
        "#@markdown |Name                |Description|\n",
        "#@markdown |:---                |:---|\n",
        "#@markdown |`timeout_second`    ||\n",
        "#@markdown |`max_program_length`|The maximum length of the program|\n",
        "timeout_second = 1 #@param {type: \"number\"}\n",
        "max_program_length = 3 #@param {type: \"number\"}\n",
        "\n",
        "#@markdown ### Other Settings\n",
        "#@markdown |Name    |Description|\n",
        "#@markdown |:---    |:---|\n",
        "#@markdown |`device`|The id of GPU. `-1` means that CPU is used.|\n",
        "device = 0 #@param {type: \"number\"}\n",
        "\n",
        "#@markdown ### Filepath\n",
        "#@markdown |Name                |Description|\n",
        "#@markdown |:---                |:---|\n",
        "#@markdown |`train_dataset_path`|The file path of the training dataset.|\n",
        "#@markdown |`valid_dataset_path`|The file path of the validation dataset.|\n",
        "#@markdown |`destination_path`  |The directory of the directory that will contain the training results.|\n",
        "train_dataset_path = \"/gdrive/My Drive/DeepCoder/dataset/length_3/train.pickle\" #@param {type: \"string\"}\n",
        "valid_dataset_path = \"/gdrive/My Drive/DeepCoder/dataset/length_3/valid.pickle\" #@param {type: \"string\"}\n",
        "destination_path = \"/gdrive/My Drive/DeepCoder/out/length_3/w0_0.5\" #@param {type: \"string\"}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BembldCdOO1",
        "colab_type": "text"
      },
      "source": [
        "### Setup\n",
        "* Fix the random seed\n",
        "* Download the codebase\n",
        "  1. Clone git repository and move to the specified branch\n",
        "  2. Initialize submodule\n",
        "  3. Install chainer and cupy\n",
        "* Copy the dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwjlAkY1fR5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "SEED_MAX = 2**32 - 1\n",
        "\n",
        "root_rng = np.random.RandomState(seed)\n",
        "random.seed(root_rng.randint(SEED_MAX))\n",
        "np.random.seed(root_rng.randint(SEED_MAX))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIZJmuz8QFn_",
        "colab_type": "code",
        "outputId": "5a7a6ed8-ed61-4c52-cddf-becb723e80a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "if runtime == \"host\":\n",
        "  %cd /content\n",
        "  !rm -rf deep-coder\n",
        "  ![ ! -e deep-coder ] && git clone $repository_url deep-coder\n",
        "  %cd deep-coder\n",
        "  !git checkout origin/$branch_name\n",
        "  !git submodule init\n",
        "  !git submodule update\n",
        "  !make -C DeepCoder_Utils/enumerative-search -j `nproc`\n",
        "  !curl https://colab.chainer.org/install | sh -\n",
        "  !pip install tqdm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'deep-coder'...\n",
            "remote: Enumerating objects: 144, done.\u001b[K\n",
            "remote: Counting objects: 100% (144/144), done.\u001b[K\n",
            "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
            "remote: Total 1268 (delta 83), reused 64 (delta 34), pack-reused 1124\u001b[K\n",
            "Receiving objects: 100% (1268/1268), 17.84 MiB | 13.96 MiB/s, done.\n",
            "Resolving deltas: 100% (761/761), done.\n",
            "/content/deep-coder\n",
            "Note: checking out 'origin/master'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at cbf6f06 Add example script to generate the dataset\n",
            "Submodule 'DeepCoder_Utils' (https://github.com/HiroakiMikami/DeepCoder-Utils.git) registered for path 'DeepCoder_Utils'\n",
            "Cloning into '/content/deep-coder/DeepCoder_Utils'...\n",
            "Submodule path 'DeepCoder_Utils': checked out '10330caf96b2f6bf354c512010b356a7b0d1dba5'\n",
            "make: Entering directory '/content/deep-coder/DeepCoder_Utils/enumerative-search'\n",
            "g++ -std=c++11 -O3   successor.cc -c -o successor.o\n",
            "g++ -std=c++11 -O3   ops.cc -c -o ops.o\n",
            "g++ -std=c++11 -O3   program_state.cc -c -o program_state.o\n",
            "g++ -std=c++11 -O3   main.cc -c -o main.o\n",
            "g++ -std=c++11 -O3   datum.cc -c -o datum.o\n",
            "g++ -std=c++11 -O3   depth_first_search.cc -c -o depth_first_search.o\n",
            "g++ -std=c++11 -O3   utils.cc -c -o utils.o\n",
            "g++ -std=c++11 -O3   io_set.cc -c -o io_set.o\n",
            "g++ -std=c++11 -O3  successor.o ops.o program_state.o main.o datum.o depth_first_search.o utils.o io_set.o -o search\n",
            "make: Leaving directory '/content/deep-coder/DeepCoder_Utils/enumerative-search'\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1580  100  1580    0     0   1008      0  0:00:01  0:00:01 --:--:--  1007\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "+ set +ex\n",
            "Installation succeeded!\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz7sdzxUi70b",
        "colab_type": "text"
      },
      "source": [
        "### Train DNN Model\n",
        "* Create `Trainer`\n",
        "* Run training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7kdglcUjDTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "import chainer as ch\n",
        "from chainer import datasets\n",
        "from chainer.training import extensions\n",
        "from src.dataset import EncodedDataset, Dataset\n",
        "import src.train as T\n",
        "from src.model import ModelShapeParameters\n",
        "\n",
        "with open(train_dataset_path, \"rb\") as f:\n",
        "    d: Dataset = pickle.load(f)\n",
        "dataset = d.dataset\n",
        "metadata = d.metadata\n",
        "    \n",
        "\n",
        "if num_train != 0:\n",
        "    num_test = int(num_train *\n",
        "                   (ratio_test if ratio_test is not None else 0.0))\n",
        "    dataset, _ = datasets.split_dataset_random(\n",
        "        dataset, num_train + num_test, seed=root_rng.randint(SEED_MAX))\n",
        "\n",
        "model_shape = ModelShapeParameters(metadata, num_hidden_layers, n_embed, n_units)\n",
        "\n",
        "n_entries = len(dataset)\n",
        "dataset = EncodedDataset(Dataset(dataset, metadata))\n",
        "if ratio_test is None or ratio_test == 0:\n",
        "    train = dataset\n",
        "    test = None\n",
        "else:\n",
        "    train, test = datasets.split_dataset_random(dataset, int(\n",
        "        n_entries * (1.0 - ratio_test)), seed=root_rng.randint(SEED_MAX))\n",
        "\n",
        "train_iter = ch.iterators.SerialIterator(train, batch_size)\n",
        "if test is not None:\n",
        "    test_iter = ch.iterators.SerialIterator(\n",
        "        test, batch_size, repeat=False, shuffle=False)\n",
        "else:\n",
        "    test_iter = None\n",
        "\n",
        "train = T.Training(train_iter, test_iter, destination_path, model_shape, weight_label_false,\n",
        "                   num_epochs, device=device)\n",
        "train.trainer.extend(extensions.LogReport())\n",
        "if test_iter is not None:\n",
        "    train.trainer.extend(extensions.PrintReport(\n",
        "        ['epoch',\n",
        "         'main/loss', 'validation/main/loss',\n",
        "         'main/accuracy', 'main/accuracy_false', 'main/accuracy_true',\n",
        "         'validation/main/accuracy', 'validation/main/accuracy_false', 'validation/main/accuracy_true',\n",
        "         'elapsed_time']))\n",
        "else:\n",
        "    train.trainer.extend(extensions.PrintReport(\n",
        "        ['epoch', 'main/loss', 'main/accuracy', 'main/accuracy_false', 'main/accuracy_true', 'elapsed_time']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl4xN2N2kGfo",
        "colab_type": "code",
        "outputId": "a15e65b7-c809-4637-ac2a-a98c0d83b54e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train.trainer.run()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch       main/loss   main/accuracy  main/accuracy_false  main/accuracy_true  elapsed_time\n",
            "\u001b[J1           0.423574    0.781757       0.77777              0.806946            20.6041       \n",
            "\u001b[J2           0.36427     0.808729       0.802518             0.848036            40.6599       \n",
            "\u001b[J3           0.335445    0.823957       0.817089             0.867429            60.5894       \n",
            "\u001b[J4           0.314533    0.834011       0.826772             0.879794            80.5645       \n",
            "\u001b[J5           0.29805     0.841023       0.833164             0.890762            100.814       \n",
            "\u001b[J6           0.283511    0.846967       0.838505             0.900508            120.943       \n",
            "\u001b[J7           0.269242    0.852813       0.843739             0.910211            141.312       \n",
            "\u001b[J8           0.254964    0.858402       0.848644             0.920211            161.366       \n",
            "\u001b[J9           0.241837    0.863954       0.853748             0.928533            181.694       \n",
            "\u001b[J10          0.228309    0.869931       0.859263             0.937446            201.765       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sl37YHR_b6L",
        "colab_type": "text"
      },
      "source": [
        "### Save DNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-WYlqxVkO5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import chainer as ch\n",
        "\n",
        "if not os.path.exists(destination_path):\n",
        "    os.makedirs(destination_path)\n",
        "\n",
        "with open(os.path.join(destination_path, \"model-shape.pickle\"), \"wb\") as f:\n",
        "    pickle.dump(model_shape, f)\n",
        "\n",
        "ch.serializers.save_npz(os.path.join(destination_path, \"model.npz\"), train.predictor)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4IOCX_PXG6sH"
      },
      "source": [
        "### Validate DNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SsVdGBe4G6sJ",
        "outputId": "bd8d1172-2fd4-4899-8260-bc8ef948972f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "import chainer as ch\n",
        "from chainer import datasets\n",
        "from src.dataset import EncodedDataset, Dataset\n",
        "import src.inference as I\n",
        "from src.model import ModelShapeParameters\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "model = I.InferenceModel(model_shape)\n",
        "ch.serializers.load_npz(os.path.join(destination_path, \"model.npz\"), model.predictor)\n",
        "\n",
        "with open(valid_dataset_path, \"rb\") as f:\n",
        "    dataset: Dataset = pickle.load(f)\n",
        "\n",
        "pred = I.predict_with_neural_network(model_shape, model)\n",
        "\n",
        "results = dict([])\n",
        "num_succ = 0\n",
        "for i, (entry,) in enumerate(tqdm(dataset.dataset)):\n",
        "    result = I.search(\n",
        "        os.path.join(os.getcwd(), \"DeepCoder_Utils\",\n",
        "                     \"enumerative-search\", \"search\"),\n",
        "        timeout_second,\n",
        "        model_shape.dataset_metadata.value_range,\n",
        "        entry.examples,\n",
        "        max_program_length,\n",
        "        pred\n",
        "    )\n",
        "    results[i] = result\n",
        "    if result.is_solved:\n",
        "        num_succ += 1\n",
        "\n",
        "print(\"Solved: {} of {} examples\".format(num_succ, len(dataset.dataset)))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "befca3e5657a470fb4c4a4b7e47886c4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Solved: 470 of 500 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lhq-S-vcGxUQ"
      },
      "source": [
        "### Save Validation Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DuxS691_fuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(destination_path):\n",
        "    os.makedirs(destination_path)\n",
        "\n",
        "with open(os.path.join(destination_path, \"result.pickle\"), \"wb\") as f:\n",
        "    pickle.dump(results, f)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}